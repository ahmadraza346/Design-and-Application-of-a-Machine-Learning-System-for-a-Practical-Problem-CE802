{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "import pandas as pd\n",
    "import math\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, mean_squared_error,r2_score\n",
    "from sklearn.model_selection import cross_validate, KFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR\n",
    "#Read CSV file into DataFrame train_data\n",
    "train_data = pd.read_csv('CE802_P3_Data.csv')\n",
    "X1=train_data.drop(columns=['Target'])\n",
    "Y1=train_data['Target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1     0\n",
      "F2     0\n",
      "F3     0\n",
      "F4     0\n",
      "F5     0\n",
      "F6     0\n",
      "F7     0\n",
      "F8     0\n",
      "F9     0\n",
      "F10    0\n",
      "F11    0\n",
      "F12    0\n",
      "F13    0\n",
      "F14    0\n",
      "F15    0\n",
      "F16    0\n",
      "F17    0\n",
      "F18    0\n",
      "F19    0\n",
      "F20    0\n",
      "F21    0\n",
      "F22    0\n",
      "F23    0\n",
      "F24    0\n",
      "F25    0\n",
      "F26    0\n",
      "F27    0\n",
      "F28    0\n",
      "F29    0\n",
      "F30    0\n",
      "F31    0\n",
      "F32    0\n",
      "F33    0\n",
      "F34    0\n",
      "F35    0\n",
      "F36    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X1.isnull().sum())  #no null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking for the datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['F4', 'F9'], dtype='object')"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_F = X1.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_F = X1.select_dtypes(include=['object']).columns\n",
    "categorical_F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As two columns have the categorical values, now we will check how many type of categories they contain and perform their encoding accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Europe    393\n",
       "UK        378\n",
       "Rest      365\n",
       "USA       364\n",
       "Name: F4, dtype: int64"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1['F4'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "High         307\n",
       "Very low     307\n",
       "Very high    306\n",
       "Medium       294\n",
       "Low          286\n",
       "Name: F9, dtype: int64"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1['F9'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that both features contain 4 and 5 type of values respectively, therefore we will be using one hot encoding to get rid of the categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F10</th>\n",
       "      <th>F11</th>\n",
       "      <th>F12</th>\n",
       "      <th>...</th>\n",
       "      <th>F34</th>\n",
       "      <th>F35</th>\n",
       "      <th>F36</th>\n",
       "      <th>F4_Rest</th>\n",
       "      <th>F4_UK</th>\n",
       "      <th>F4_USA</th>\n",
       "      <th>F9_Low</th>\n",
       "      <th>F9_Medium</th>\n",
       "      <th>F9_Very high</th>\n",
       "      <th>F9_Very low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-69.21</td>\n",
       "      <td>16536.84</td>\n",
       "      <td>6</td>\n",
       "      <td>-16720.42</td>\n",
       "      <td>735.74</td>\n",
       "      <td>22.14</td>\n",
       "      <td>15.48</td>\n",
       "      <td>124.05</td>\n",
       "      <td>1618.89</td>\n",
       "      <td>-1.19</td>\n",
       "      <td>...</td>\n",
       "      <td>4.06</td>\n",
       "      <td>40.32</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-168.67</td>\n",
       "      <td>28434.21</td>\n",
       "      <td>12</td>\n",
       "      <td>-8070.93</td>\n",
       "      <td>91.35</td>\n",
       "      <td>-1.86</td>\n",
       "      <td>18.60</td>\n",
       "      <td>57.78</td>\n",
       "      <td>1137.78</td>\n",
       "      <td>-9.45</td>\n",
       "      <td>...</td>\n",
       "      <td>33.52</td>\n",
       "      <td>176.98</td>\n",
       "      <td>5.81</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-76.19</td>\n",
       "      <td>22895.97</td>\n",
       "      <td>18</td>\n",
       "      <td>-12126.02</td>\n",
       "      <td>145.64</td>\n",
       "      <td>-68.28</td>\n",
       "      <td>14.22</td>\n",
       "      <td>-65.88</td>\n",
       "      <td>2065.89</td>\n",
       "      <td>-9.65</td>\n",
       "      <td>...</td>\n",
       "      <td>28.90</td>\n",
       "      <td>165.00</td>\n",
       "      <td>2.85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-103.19</td>\n",
       "      <td>22926.51</td>\n",
       "      <td>12</td>\n",
       "      <td>-10050.95</td>\n",
       "      <td>218.39</td>\n",
       "      <td>-40.58</td>\n",
       "      <td>14.99</td>\n",
       "      <td>132.12</td>\n",
       "      <td>3228.42</td>\n",
       "      <td>-10.63</td>\n",
       "      <td>...</td>\n",
       "      <td>1.12</td>\n",
       "      <td>211.88</td>\n",
       "      <td>2.96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-49.84</td>\n",
       "      <td>-4224.12</td>\n",
       "      <td>6</td>\n",
       "      <td>-10197.84</td>\n",
       "      <td>-346.17</td>\n",
       "      <td>-47.04</td>\n",
       "      <td>8.92</td>\n",
       "      <td>113.31</td>\n",
       "      <td>1678.11</td>\n",
       "      <td>-8.72</td>\n",
       "      <td>...</td>\n",
       "      <td>8.16</td>\n",
       "      <td>154.32</td>\n",
       "      <td>2.92</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>-50.50</td>\n",
       "      <td>25230.48</td>\n",
       "      <td>9</td>\n",
       "      <td>-10631.66</td>\n",
       "      <td>456.24</td>\n",
       "      <td>-22.64</td>\n",
       "      <td>14.79</td>\n",
       "      <td>98.16</td>\n",
       "      <td>-122.91</td>\n",
       "      <td>-8.15</td>\n",
       "      <td>...</td>\n",
       "      <td>3.56</td>\n",
       "      <td>174.96</td>\n",
       "      <td>5.04</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>-131.43</td>\n",
       "      <td>33310.62</td>\n",
       "      <td>24</td>\n",
       "      <td>-7262.50</td>\n",
       "      <td>653.10</td>\n",
       "      <td>33.66</td>\n",
       "      <td>16.25</td>\n",
       "      <td>52.11</td>\n",
       "      <td>1906.05</td>\n",
       "      <td>-9.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.58</td>\n",
       "      <td>231.90</td>\n",
       "      <td>6.37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>-31.35</td>\n",
       "      <td>22501.02</td>\n",
       "      <td>12</td>\n",
       "      <td>-11386.26</td>\n",
       "      <td>-215.22</td>\n",
       "      <td>11.04</td>\n",
       "      <td>14.43</td>\n",
       "      <td>34.23</td>\n",
       "      <td>2200.11</td>\n",
       "      <td>-11.66</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06</td>\n",
       "      <td>161.12</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>-67.45</td>\n",
       "      <td>21919.62</td>\n",
       "      <td>18</td>\n",
       "      <td>-8660.33</td>\n",
       "      <td>632.65</td>\n",
       "      <td>30.34</td>\n",
       "      <td>16.21</td>\n",
       "      <td>100.92</td>\n",
       "      <td>677.82</td>\n",
       "      <td>-9.31</td>\n",
       "      <td>...</td>\n",
       "      <td>64.20</td>\n",
       "      <td>351.34</td>\n",
       "      <td>4.97</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>-79.15</td>\n",
       "      <td>21329.70</td>\n",
       "      <td>24</td>\n",
       "      <td>-7895.09</td>\n",
       "      <td>32.68</td>\n",
       "      <td>10.44</td>\n",
       "      <td>16.97</td>\n",
       "      <td>93.21</td>\n",
       "      <td>2369.91</td>\n",
       "      <td>-17.44</td>\n",
       "      <td>...</td>\n",
       "      <td>14.28</td>\n",
       "      <td>72.58</td>\n",
       "      <td>4.39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          F1        F2  F3        F5      F6     F7     F8     F10      F11  \\\n",
       "0     -69.21  16536.84   6 -16720.42  735.74  22.14  15.48  124.05  1618.89   \n",
       "1    -168.67  28434.21  12  -8070.93   91.35  -1.86  18.60   57.78  1137.78   \n",
       "2     -76.19  22895.97  18 -12126.02  145.64 -68.28  14.22  -65.88  2065.89   \n",
       "3    -103.19  22926.51  12 -10050.95  218.39 -40.58  14.99  132.12  3228.42   \n",
       "4     -49.84  -4224.12   6 -10197.84 -346.17 -47.04   8.92  113.31  1678.11   \n",
       "...      ...       ...  ..       ...     ...    ...    ...     ...      ...   \n",
       "1495  -50.50  25230.48   9 -10631.66  456.24 -22.64  14.79   98.16  -122.91   \n",
       "1496 -131.43  33310.62  24  -7262.50  653.10  33.66  16.25   52.11  1906.05   \n",
       "1497  -31.35  22501.02  12 -11386.26 -215.22  11.04  14.43   34.23  2200.11   \n",
       "1498  -67.45  21919.62  18  -8660.33  632.65  30.34  16.21  100.92   677.82   \n",
       "1499  -79.15  21329.70  24  -7895.09   32.68  10.44  16.97   93.21  2369.91   \n",
       "\n",
       "        F12  ...    F34     F35   F36  F4_Rest  F4_UK  F4_USA  F9_Low  \\\n",
       "0     -1.19  ...   4.06   40.32  3.18        0      0       0       0   \n",
       "1     -9.45  ...  33.52  176.98  5.81        0      1       0       0   \n",
       "2     -9.65  ...  28.90  165.00  2.85        0      0       0       0   \n",
       "3    -10.63  ...   1.12  211.88  2.96        0      0       0       0   \n",
       "4     -8.72  ...   8.16  154.32  2.92        1      0       0       0   \n",
       "...     ...  ...    ...     ...   ...      ...    ...     ...     ...   \n",
       "1495  -8.15  ...   3.56  174.96  5.04        1      0       0       0   \n",
       "1496  -9.12  ...   0.58  231.90  6.37        0      0       1       0   \n",
       "1497 -11.66  ...   0.06  161.12  1.84        0      0       0       0   \n",
       "1498  -9.31  ...  64.20  351.34  4.97        0      1       0       1   \n",
       "1499 -17.44  ...  14.28   72.58  4.39        0      0       0       0   \n",
       "\n",
       "      F9_Medium  F9_Very high  F9_Very low  \n",
       "0             1             0            0  \n",
       "1             0             1            0  \n",
       "2             0             0            0  \n",
       "3             0             1            0  \n",
       "4             0             0            0  \n",
       "...         ...           ...          ...  \n",
       "1495          0             0            1  \n",
       "1496          0             0            0  \n",
       "1497          1             0            0  \n",
       "1498          0             0            0  \n",
       "1499          0             0            0  \n",
       "\n",
       "[1500 rows x 41 columns]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1 = pd.get_dummies(X1,columns=['F4','F9'],drop_first=True)\n",
    "X1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using linear regression without feature engineering, hyper-parameter tuning, cross-validation and pipelining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error:\n",
      "657.1515967545505\n",
      "Co efficient of determination:\n",
      "0.686809135369201\n"
     ]
    }
   ],
   "source": [
    "#splitting the data\n",
    "X, X_test, Y, Y_test = train_test_split(X1, Y1, train_size=0.8, random_state=1)\n",
    "#scaling\n",
    "scalar=StandardScaler()\n",
    "X=scalar.fit_transform(X)\n",
    "X_test=scalar.transform(X_test)\n",
    "#model training\n",
    "lr=LinearRegression()\n",
    "lr.fit(X,Y)\n",
    "#predicting values\n",
    "y_pred = lr.predict(X_test)\n",
    "#evaluating model using rmse and co efficient of determination\n",
    "RMSE = mean_squared_error(Y_test, y_pred,squared=False)\n",
    "cod=r2_score(Y_test, y_pred)\n",
    "print(\"Root Mean Square Error:\")\n",
    "print(RMSE)\n",
    "print(\"Co efficient of determination:\")\n",
    "print(cod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using linear regression with  hyper-parameter tuning, cross-validation and pipelining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__copy_X': True, 'clf__fit_intercept': True, 'clf__normalize': False, 'clf__positive': False}\n",
      "croos validation rmse -709.3924214551866\n",
      "Root Mean Square Error:\n",
      "657.1515967545505\n",
      "Co efficient of determination:\n",
      "0.686809135369201\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#splitting the data\n",
    "X, X_test, Y, Y_test = train_test_split(X1, Y1, train_size=0.80, random_state=1)                                        \n",
    "#defining the number of folds in cross-validation\n",
    "cv = KFold(n_splits=5,shuffle=True)\n",
    "#defining my pipeline\n",
    "scalar=StandardScaler()\n",
    "clf=LinearRegression(n_jobs=-1)\n",
    "pipe = Pipeline([('scalar',scalar), ('clf', clf)])\n",
    "#performing grid-search to tune-hyper-parameters\n",
    "param_grid = {'clf__fit_intercept':[True, False],'clf__normalize':[True,False],'clf__copy_X' :[True,False],'clf__positive' :[True,False]} \n",
    "search = GridSearchCV(pipe, param_grid, n_jobs=-1)\n",
    "#fitting the pipeline\n",
    "search.fit(X,Y)\n",
    "print(search.best_params_)\n",
    "#computing the average accuracy score while performing cross-validation on different folds\n",
    "scores = cross_validate(search, X, Y, scoring=['neg_root_mean_squared_error'], cv=cv, return_estimator=True)\n",
    "print('croos validation rmse', np.mean(scores['test_neg_root_mean_squared_error']))\n",
    "#predicting the values on the test data\n",
    "#pipes.fit_transform(X_test)\n",
    "y_pred = search.predict(X_test)\n",
    "#Analyzing the results\n",
    "RMSE = mean_squared_error(Y_test, y_pred,squared=False)\n",
    "cod=r2_score(Y_test, y_pred)\n",
    "print(\"Root Mean Square Error:\")\n",
    "print(RMSE)\n",
    "print(\"Co efficient of determination:\")\n",
    "print(cod)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using random forest  regression without feature engineering, hyper-parameter tuning, cross-validation and pipelining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error:\n",
      "894.9117707020526\n",
      "Co efficient of determination:\n",
      "0.4191842408011046\n"
     ]
    }
   ],
   "source": [
    "#splitting the data\n",
    "X, X_test, Y, Y_test = train_test_split(X1, Y1, train_size=0.8, random_state=1)\n",
    "#scaling\n",
    "scalar=StandardScaler()\n",
    "X=scalar.fit_transform(X)\n",
    "X_test=scalar.transform(X_test)\n",
    "#model training\n",
    "lr=RandomForestRegressor()\n",
    "lr.fit(X,Y)\n",
    "y_pred = lr.predict(X_test)\n",
    "#evaluating results based on rmse and co efficient of determination\n",
    "RMSE = mean_squared_error(Y_test, y_pred,squared=False)\n",
    "cod=r2_score(Y_test, y_pred)\n",
    "print(\"Root Mean Square Error:\")\n",
    "print(RMSE)\n",
    "print(\"Co efficient of determination:\")\n",
    "print(cod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using random forest regression with hyper-parameter tuning, cross-validation and pipelining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.48327033 0.49165035 0.49864565 ...        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__max_features': 23, 'clf__min_samples_split': 0.1, 'clf__n_estimators': 28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.47752517 0.45401306 0.47640471 ...        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.49911574 0.50004802 0.5041017  ...        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.47507029 0.47581204 0.4682582  ...        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.48056703 0.51451142 0.47726133 ...        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation rmse -869.4470092347508\n",
      "Root Mean Square Error:\n",
      "803.6932942414331\n",
      "Co efficient of determination:\n",
      "0.5315549650222016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.48285156 0.50427062 0.49223769 ...        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#splitting the data\n",
    "X, X_test, Y, Y_test = train_test_split(X1, Y1, train_size=0.80, random_state=1)                                        \n",
    "#defining the number of folds in cross-validation\n",
    "cv = KFold(n_splits=5,shuffle=True)\n",
    "#defining my pipeline\n",
    "scalar=StandardScaler()\n",
    "clf=RandomForestRegressor()\n",
    "pipe = Pipeline([ ('scalar',scalar), ('clf', clf)])\n",
    "#performing grid-search to tune-hyper-parameters\n",
    "param_grid = {'clf__max_features':list(range(15,25)),'clf__n_estimators':list(range(10,30)),'clf__min_samples_split' :np.linspace(0.1, 3.0, 10, endpoint=True)} \n",
    "search = GridSearchCV(pipe, param_grid, n_jobs=-1)\n",
    "#fitting the pipeline\n",
    "search.fit(X,Y)\n",
    "print(search.best_params_)\n",
    "#computing the average accuracy score while performing cross-validation on different folds\n",
    "scores = cross_validate(search, X, Y, scoring=['neg_root_mean_squared_error'], cv=cv, return_estimator=True)\n",
    "print('cross validation rmse', np.mean(scores['test_neg_root_mean_squared_error']))\n",
    "#predicting the values on the test data\n",
    "#pipes.fit_transform(X_test)\n",
    "y_pred = search.predict(X_test)\n",
    "#Analyzing the results\n",
    "RMSE = mean_squared_error(Y_test, y_pred,squared=False)\n",
    "cod=r2_score(Y_test, y_pred)\n",
    "print(\"Root Mean Square Error:\")\n",
    "print(RMSE)\n",
    "print(\"Co efficient of determination:\")\n",
    "print(cod)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using support vector regression without hyper-parameter tuning, cross-validation and pipelining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error:\n",
      "837.6795531378846\n",
      "Co efficient of determination:\n",
      "0.49109844183743934\n"
     ]
    }
   ],
   "source": [
    "#splitting the data\n",
    "X, X_test, Y, Y_test = train_test_split(X1, Y1, train_size=0.8, random_state=1)\n",
    "scalar=StandardScaler()\n",
    "X=scalar.fit_transform(X)\n",
    "X_test=scalar.transform(X_test)\n",
    "r=SVR(kernel='linear')\n",
    "r.fit(X,Y)\n",
    "y_pred = r.predict(X_test)\n",
    "RMSE = mean_squared_error(Y_test, y_pred,squared=False)\n",
    "cod=r2_score(Y_test, y_pred)\n",
    "print(\"Root Mean Square Error:\")\n",
    "print(RMSE)\n",
    "print(\"Co efficient of determination:\")\n",
    "print(cod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using support vector regression with hyper-parameter tuning, cross-validation and pipelining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__C': 1000, 'clf__gamma': 0.01, 'clf__kernel': 'rbf'}\n",
      "cross validation rmse -682.7191228788854\n",
      "Root Mean Square Error:\n",
      "653.3638862037872\n",
      "Co efficient of determination:\n",
      "0.6904090888660912\n"
     ]
    }
   ],
   "source": [
    "#splitting the data\n",
    "X, X_test, Y, Y_test = train_test_split(X1, Y1, train_size=0.80, random_state=1)                                        \n",
    "#defining the number of folds in cross-validation\n",
    "cv = KFold(n_splits=5,shuffle=True)\n",
    "#defining my pipeline\n",
    "scalar=StandardScaler()\n",
    "clf=SVR(kernel='rbf')\n",
    "pipe = Pipeline([ ('scalar',scalar), ('clf', clf)])\n",
    "#performing grid-search to tune-hyper-parameters\n",
    "param_grid = {'clf__C': [0.01, 0.1, 1, 10, 100, 1000], 'clf__gamma': [ 0.0001, 0.001, 0.01, 0.1, 1], 'clf__kernel': ['rbf' , 'linear']} \n",
    "search = GridSearchCV(pipe, param_grid, n_jobs=-1)\n",
    "#fitting the pipeline\n",
    "search.fit(X,Y)\n",
    "print(search.best_params_)\n",
    "#computing the average accuracy score while performing cross-validation on different folds\n",
    "scores = cross_validate(search, X, Y, scoring=['neg_root_mean_squared_error'], cv=cv, return_estimator=True)\n",
    "print('cross validation rmse', np.mean(scores['test_neg_root_mean_squared_error']))\n",
    "#predicting the values on the test data\n",
    "#pipes.fit_transform(X_test)\n",
    "y_pred = search.predict(X_test)\n",
    "#Analyzing the results\n",
    "RMSE = mean_squared_error(Y_test, y_pred,squared=False)\n",
    "cod=r2_score(Y_test, y_pred)\n",
    "print(\"Root Mean Square Error:\")\n",
    "print(RMSE)\n",
    "print(\"Co efficient of determination:\")\n",
    "print(cod)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing Feature Engineering by using the confusionmatrix method to find which features are co-related.\n",
    "On performing this method, it can been seen that no feature is co-related to the other feature even at 50 percent. \n",
    "So, no features can be dropped.\n",
    "Note: Code is used from Project-pro\n",
    "https://www.projectpro.io/recipes/drop-out-highly-correlated-features-in-python#:~:text=%20How%20to%20drop%20out%20highly%20correlated%20features,trigular%20matrix.%20So%20now%20we%20are...%20More%20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "correlation = X1.corr().abs()\n",
    "upper_triangle = correlation.where(np.triu(np.ones(correlation.shape),k=1).astype(bool))\n",
    "#dropping columns having correlation greater than 90 percent\n",
    "dropping = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.70)]\n",
    "print(dropping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the features importance now in the best performing algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12872/1226540901.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mFX1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscalar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSVR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'linear'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mimportance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimportance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimportance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupport_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupport_vectors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_support\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual_coef_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_probA\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m             self._probB, self.fit_status_ = libsvm.fit(\n\u001b[0m\u001b[0;32m    278\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m                 \u001b[0msvm_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scalar=StandardScaler()\n",
    "FX1=scalar.fit_transform(X1)\n",
    "model=SVR(kernel='linear')\n",
    "model=model.fit(X1,Y1)\n",
    "importance = model.feature_importances_\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on least RMSE algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X11=train_data.drop(columns=['Target'])\n",
    "Y11=train_data['Target']\n",
    "X11 = pd.get_dummies(X11,columns=['F4','F9'],drop_first=True)\n",
    "scalar=StandardScaler()\n",
    "X11=scalar.fit_transform(X11)\n",
    "rfc=SVR(C=1000,gamma=0.01,kernel='rbf')\n",
    "rfc.fit(X11,Y11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Values on Actual Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_df = pd.read_csv('CE802_P3_Test.csv')\n",
    "test_data = pd.read_csv('CE802_P3_Test.csv')\n",
    "X1=test_data.drop(columns=['Target'])\n",
    "Y1=test_data['Target']\n",
    "X1 = pd.get_dummies(X1,columns=['F4','F9'],drop_first=True)\n",
    "scalar=StandardScaler()\n",
    "X1=scalar.fit_transform(X1)\n",
    "predicted = rfc.predict(X1)\n",
    "test_df.iloc[:,-1] = predicted\n",
    "test_df.to_csv('CE802_P3_Test_Predictions.csv', index=False, float_format='%.8g')\n",
    "assert pd.read_csv('CE802_P3_Test.csv').iloc[:,:-1].equals(pd.read_csv('CE802_P3_Test_Predictions.csv').iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
